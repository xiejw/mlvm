\input opmac

% Increase the gap between paragraphs.
\parskip=1\baselineskip

% Define a special font fo sub title.
\font\tensf=cmss10
\let\subtitlefont=\tensf

% Define macors for stride markder.
\def\strideleft{\backslash\{}
\def\strideright{\backslash\}}

% Call the indentation for descriptions \descindent and set it to 6 picas.
\newdimen\descindent \descindent = 6pc
\def\itembox#1{\noindent\llap{\hbox to \descindent{\bf #1\hfil}}}

%%%%%

\noindent {\subtitlefont From user's view}, Tensor a multi-dimension float-value
array. For a Tensor with shape $<a, b, c>$, where $a,b,$ and $c$ are all
positive integers, there are in total of $ a b c $ number of elements, called
{\tt size}, in it.  The length of the shape is called {\tt rank}. It is also the
number of dimensions. Scalar is viewed as a Tensor with shape $<1>$, i.e., rank
$1$.

\noindent This leads to the following definition:

\begtt
typedef struct {
  mlvm_uint_t  rank;   /* Must be positive (non-zero). */
  mlvm_uint_t* shape;  /* Length is `rank` above. */
  mlvm_size_t  size;   /* Total number of elements. */
  double*      value;  /* The value buffer. */
} tensor_t;
\endtt

\noindent {\subtitlefont From intermediate representation's view}, there are two
new concepts.

{\leftskip=\descindent

  \itembox{value mode}{}The first one is ownership, called {\tt value-mode\_},
  which is an internal field. It represents whether the current Tensor owns its
  value buffer.

  \itembox{stride}{}The second one is {\tt stride}, with type {\tt
  mlvm\_size\_t*}.  It is an array with same length as {\tt shape}. For {\tt
  stride[i]}, it represents the offset difference between the dimension
  increment at dimension {\tt shape[i]}.

  {\leftskip=2\leftskip

    \itembox{tranpose}{}For a Tensor with shape $<a,b, c>$ and stride
    $\strideleft bc, c, 1]$, {\it tranpose(1, 2, 0)} produces a new Tensor with
    shape $<b, c, a>$. With {\tt stride} property, we could re-use the same
    value buffer, saving the copy cost, and represent the new Tensor as: shape
    $<b, c, a>$ and stride $\strideleft c, 1, bc]$.

    \itembox{broadcast}{}For a Tensor with shape $<a, 1>$ and stride
    $\strideleft 1, 1\strideright$, {\it broadcast\/}ing to $<a,b>$ is cost free
    as we just need a new Tensor with shape $<a,b>$ and stride $\strideleft 1,
    0\strideright$. Similary, {\it broadcast\/}ing a Tensor with shape $<1, b>$
    to new shape $<a,b>$ can be represented as shape $<a,b>$ and stride
    $\strideleft 0, 1\strideright$.

    \itembox{constant}{}If all above make sense, a {\tt zeros} tensor with shape
    $<a,b>$ is simply {\it broadcast\/}ing a scalar $0$ to $<a,b>$, i.e., stride
    $\strideleft 0, 0\strideright$. So is a {\tt ones} tensor.

    \itembox{data access}{}This would be a big {\it disadvantage}.  Locality is
    bad for arbitrary stride. Iterating the data value is also quite
    complicated.

    \itembox{reshape}{}This would be another {\it disadvantage}.  {\it
    Reshap\/}ing a Tensor with arbitrary stride is not always cost-free.  For
    example, {\it reshap\/}ing a Tensor with shape $<a,b>$ and stride
    $\strideleft 1, 0\strideright$ to $<ab>$ needs a value buffer copy.

  \par}
\par}

\bye
