\input opmac

% Increase the gap between paragraphs.
\parskip=1\baselineskip

% Define a special font fo sub title.
\font\tensf=cmss10
\let\subtitlefont=\tensf

\def\subtitle#1{\noindent {\subtitlefont #1}\par}

% Define the conv binary op
\def\conv{\otimes}

% Call the indentation for descriptions \descindent and set it to 6 picas.
\newdimen\descindent \descindent = 6pc
\def\itembox#1{\noindent\llap{\hbox to \descindent{\bf #1\hfil}}}


%%%%%

\subtitle{Convolution and its gradients}

\noindent Conv2D operation and/or layer in machine learning can be expressed as
many small convolution kernels, followed by adding biases.

\noindent Let's first define a convolution kernel in its simplied form. Given
$X, y$, and $W$, where $X$ and $y$ are the input matrix with shape $<m,n>$ and
scalar output respectively and $W$ is the filter with shape $<a,b>$, we define
the convolution kernel
%
$$
y = X \odot W = \sum_{x=0}^{a-1} \sum_{y=0}^{b-1} X_{x,y} W_{x,y}
$$
%
as a two-dimension dot product over all non-zero elements where the top-left
corners of $X$ and $W$ are aligned. In addition, if $a > m$ or $b>n$, the $X$ is
padded with zeros.

\noindent With this, gradients are straightforward:
%
$$
\eqalign{
  {\hbox{d} y \over \hbox{d}W_{x,y}} & = X_{x,y} \cr
  {\hbox{d} y \over \hbox{d}X_{x,y}} & = W_{x,y}
}
$$
%

\noindent Secondly, let's define a shifted version for convolution kernel:
%
$$
y_{\triangleright\; \alpha,\beta}
    = \bigl( X \odot W \bigr)_{\triangleright\; \alpha,\beta}
    = X \odot W_{\triangleright\; \alpha,\beta}
    = \sum_{x=0}^{a-1} \sum_{y=0}^{b-1} X_{x+\alpha,y+\beta} W_{x,y}
$$
%
where ${\triangleright\; \alpha,\beta}$ means shifting to right and down with
$(\alpha, \beta)$ offsets. Any missing values due to out of boundaries are
padded with zeros. Accordingly, gradients are as follows:
%
$$
\eqalign{
  {\hbox{d} y_{\triangleright\; \alpha,\beta} \over \hbox{d}W_{x,y}} &
      = X_{x+\alpha,y+\beta} \cr
  {\hbox{d} y_{\triangleright\; \alpha,\beta} \over \hbox{d}X_{x,y}} &
      = W_{x-\alpha,y-\beta}
}
$$
%

\subtitle{Conv2D and its Gradients}

{\leftskip=\descindent

\itembox{Forward Pass}%
%
Conv2D is quite simple. Given $X, W$, and $Y$, where $X$ and $Y$ are
the input and output respectively and $W$ is a typically quite small filter, we
denote the formula as
%
$$ Y = X \conv W. $$

\noindent There is a hidden controlling argument for the Conv2D, which is the
padding. Assuming the padding is {\tt same}, which means the shape of the
output $Y$ is same as the shape of input $X$, denocated as $<m, n>$ and, $W$ has
odd number of rows and columns, denoted as $<a,b>$, then we have
%
$$
\eqalign{
  Y_{i,j}
    & = y _{\triangleright\;i, j} \cr
    & = (X \odot W)_{\triangleright\;i - {a-1\over2},j-{b-1\over2}} \cr
    & = \sum_{x=0}^{a-1} \sum_{y=0}^{b-1}
        X_{i+x-{a-1 \over 2},j+y-{b-1 \over 2}} W_{x,y}.
}
$$
where $0\le i \le m-1$ and $0\le j \le n -1$. Here, there is a hidden padding in
the input $X$, i.e., for any coordinates out of the boundaries, the input value
is zero.

\itembox{Grad of~$X$}%
%
If we examine the Jacobian matrix, each column, i.e., for a fixed input $x$, has
$ab$ non-zero elements, due to the filter shape $<a,b>$. And due to the
symmetricity, it is trivial to prove:
%
$$ dX = dY \conv W'. $$
where the padding is still {\tt same} and $W'$ is a matrix with same shape
$<a,b>$ as $W$, but all elements reversed, i.e.,
$$ W' =
\left[
  \matrix{
    W_{a-1,b-1} & W_{a-1, b-2} & \ldots & W_{a-1, 0} \cr
    W_{a-2,b-1} & W_{a-2, b-2} & \ldots & W_{a-2, 0} \cr
    \vdots      & \vdots       & \ddots & \vdots \cr
    W_{0,b-1}   & W_{0, b-2}   & \ldots & W_{0, 0}
  }
\right]
$$

\itembox{Grad of~$W$}%
%
For the central point of $W$, the gradient is
%
$$
dW_{{a-1\over2}, {b-1\over2}} = dY \odot X
$$
%
where $\odot$ is a 2-D point-wise dot product. With that, we can summerize the
gradients for each point of $W$ as
%
$$
dW_{x+{a-1\over2}, y+{b-1\over2}} = dY_{\triangleright (x,y)} \odot X
$$
where $-{a-1\over2} \le x \le {a-1\over2}, -{b-1\over2} \le y \le {b-1\over2}$
and $\triangleright (x,y)$ means shifting the matrix to right with $(x,y)$
offsets.


\par}

\bye
